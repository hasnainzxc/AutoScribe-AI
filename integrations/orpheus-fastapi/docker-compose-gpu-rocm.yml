services:
  orpheus-fastapi:
    container_name: orpheus-fastapi
    image: orpheus-tts-fastapi-server-orpheus-fastapi:latest
    build:
      context: .
      dockerfile: Dockerfile.gpu-rocm
    ports:
      - "5005:5005"
    env_file:
      - .env
    environment:
      - ORPHEUS_API_URL=http://llama-cpp-server:5006/v1/completions
      # --- Force ROCm GPU path in TTS ---
      - ORPHEUS_FORCE_GPU=1
      - HIP_VISIBLE_DEVICES=0
      - PYTORCH_ROCM_ARCH=gfx1030
      - HSA_OVERRIDE_GFX_VERSION=10.3.0
    ipc: host
    privileged: true
    security_opt:
      - seccomp=unconfined
    cap_add:
      - SYS_PTRACE
      - CAP_SYS_ADMIN
    devices:
      - /dev/kfd
      - /dev/dri
      - /dev/mem
    group_add:
      - video
      - render
    shm_size: 8g
    restart: unless-stopped
    depends_on:
      llama-cpp-server:
        condition: service_started

  llama-cpp-server:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    ports:
      - "5006:5006"
    volumes:
      - ./models:/models
    env_file:
      - .env
    environment:
      # Provide safe defaults so --batch-size/--ubatch-size never get empty
      - LLAMA_BATCH=1024
      - LLAMA_UBATCH=512
    depends_on:
      model-init:
        condition: service_completed_successfully
    cap_add:
      - SYS_PTRACE
      - CAP_SYS_ADMIN
    security_opt:
      - seccomp=unconfined
    privileged: true
    devices:
      - /dev/kfd
      - /dev/dri
      - /dev/mem
    group_add:
      - video
      - 993
    ipc: host
    shm_size: 8g
    restart: unless-stopped
    command: >
      -m /models/${ORPHEUS_MODEL_NAME}
      --port 5006
      --host 0.0.0.0
      --main-gpu 0
      --n-gpu-layers 29
      --ctx-size ${ORPHEUS_MAX_TOKENS}
      --batch-size ${LLAMA_BATCH}
      --ubatch-size ${LLAMA_UBATCH}
      --rope-scaling linear

  model-init:
    image: curlimages/curl:latest
    user: ${UID}:${GID}
    volumes:
      - ./models:/app/models
    working_dir: /app
    command: >
      sh -c '
      if [ ! -f /app/models/${ORPHEUS_MODEL_NAME} ]; then
        echo "Downloading model file..."
        curl -L -o /app/models/${ORPHEUS_MODEL_NAME} \
        https://huggingface.co/lex-au/${ORPHEUS_MODEL_NAME}/resolve/main/${ORPHEUS_MODEL_NAME}
      else
        echo "Model file already exists"
      fi'
    restart: "no"
